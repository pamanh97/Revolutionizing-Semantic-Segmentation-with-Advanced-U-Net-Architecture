# Các thư viện cần thiết
import os
import cv2
import wandb
import random
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from typing import List, Dict, Tuple, Optional, Union
import albumentations as A
from torch.cuda import amp
import torch.optim as optim
from torchmetrics import Dice
from torch.optim import RAdam
import matplotlib.pyplot as plt
import segmentation_models_pytorch as smp
from torch.utils.data import Dataset, Subset
from albumentations.pytorch import ToTensorV2
from torchmetrics.segmentation import MeanIoU, JaccardIndex
from sklearn.model_selection import KFold
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR
from torchvision.transforms.functional import to_tensor
from segmentation_models_pytorch.losses import DiceLoss, FocalLoss, LovaszLoss
from torch.utils.data import Dataset, random_split, DataLoader
from timm.models.layers import DropPath
from timm.models.efficientnet import efficientnet_b4
from albumentations import (
    HorizontalFlip, VerticalFlip, RandomRotate90, RandomBrightnessContrast, 
    GaussNoise, ElasticTransform, GridDistortion, RandomResizedCrop, 
    Normalize, Compose, ShiftScaleRotate, ColorJitter
)

# Cấu hình huấn luyện
CONFIG = {
    'seed': 42,
    'image_size': 512,
    'num_epochs': 50,
    'batch_size': 16,
    'num_workers': 4,
    'learning_rate': 1e-3,
    'weight_decay': 1e-4,
    'early_stop_patience': 8,
    'lr_scheduler': 'cosine',  # 'cosine' or 'onecycle'
    'use_mixed_precision': True,
    'k_folds': 5,
    'loss_weights': {
        'bce': 0.5,
        'dice': 0.3,
        'focal': 0.1,
        'lovasz': 0.1
    },
    'encoder_name': 'efficientnet-b4',
    'decoder_attention_type': 'scse',  # spatial and channel squeeze & excitation
    'use_deep_supervision': True,
    'use_test_time_augmentation': True
}

# Đặt seed cho tái sản xuất
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(CONFIG['seed'])

# Xóa bộ nhớ GPU nếu cần
def clear_gpu_memory():
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

clear_gpu_memory()

# Khởi tạo Weights & Biases
wandb.init(project="road-segmentation-advanced", config=CONFIG)

# Lớp tập dữ liệu nâng cấp với cân bằng lớp và xử lý dữ liệu hiệu quả
class AdvancedRoadSegmentationDataset(Dataset):
    def __init__(
        self, 
        image_dir: str, 
        mask_dir: str, 
        transform: Optional[A.Compose] = None, 
        num_images: int = -1,
        preload: bool = False
    ):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        
        # Tải và sắp xếp danh sách tệp
        all_image_list = sorted(os.listdir(image_dir))
        all_mask_list = sorted(os.listdir(mask_dir))
        
        # Đảm bảo không có sự không khớp giữa số lượng hình ảnh và mặt nạ
        assert len(all_image_list) == len(all_mask_list), "Không khớp giữa hình ảnh và mặt nạ"
        
        # Giới hạn kích thước danh sách nếu cần
        if num_images > 0:
            self.image_list = all_image_list[:min(num_images, len(all_image_list))]
            self.mask_list = all_mask_list[:min(num_images, len(all_mask_list))]
        else:
            self.image_list = all_image_list
            self.mask_list = all_mask_list
            
        # Tuỳ chọn tải trước dữ liệu vào bộ nhớ
        self.preload = preload
        self.preloaded_data = []
        
        if self.preload:
            for idx in tqdm(range(len(self.image_list)), desc="Đang tải trước dữ liệu"):
                img_path = os.path.join(self.image_dir, self.image_list[idx])
                mask_path = os.path.join(self.mask_dir, self.mask_list[idx])
                
                image = cv2.imread(img_path)
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                
                self.preloaded_data.append((image, mask))
                
        # Phân tích tổng quan về tập dữ liệu (tỷ lệ lớp)
        self.analyze_class_distribution()
    
    def analyze_class_distribution(self):
        # Phân tích phân phối lớp trong các mặt nạ
        print("Phân tích phân phối lớp trong tập dữ liệu...")
        
        sample_size = min(100, len(self.image_list))
        pixel_counts = {"foreground": 0, "background": 0}
        
        for idx in random.sample(range(len(self.image_list)), sample_size):
            if self.preload:
                mask = self.preloaded_data[idx][1]
            else:
                mask_path = os.path.join(self.mask_dir, self.mask_list[idx])
                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            
            # Đếm pixel
            pixel_counts["foreground"] += np.sum(mask > 0)
            pixel_counts["background"] += np.sum(mask == 0)
        
        total_pixels = pixel_counts["foreground"] + pixel_counts["background"]
        print(f"Tỷ lệ foreground: {pixel_counts['foreground']/total_pixels:.4f}")
        print(f"Tỷ lệ background: {pixel_counts['background']/total_pixels:.4f}")
        
        # Lưu tỷ lệ lớp để sử dụng trong hàm mất mát
        self.class_weights = {
            "foreground": total_pixels / (2 * pixel_counts["foreground"]),
            "background": total_pixels / (2 * pixel_counts["background"])
        }

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, idx):
        if self.preload:
            image, mask = self.preloaded_data[idx]
        else:
            # Tải hình ảnh
            img_path = os.path.join(self.image_dir, self.image_list[idx])
            image = cv2.imread(img_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Tải mặt nạ
            mask_path = os.path.join(self.mask_dir, self.mask_list[idx])
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        # Áp dụng biến đổi
        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        
        # Chuyển đổi thành tensors PyTorch
        if isinstance(image, np.ndarray):
            image = to_tensor(image)
            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)
        
        return image, mask

# Tăng cường dữ liệu nâng cao
def get_transforms(phase: str):
    if phase == "train":
        return A.Compose([
            A.RandomResizedCrop(height=CONFIG['image_size'], width=CONFIG['image_size'], scale=(0.8, 1.0)),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.RandomRotate90(p=0.5),
            A.ShiftScaleRotate(p=0.5),
            A.ElasticTransform(p=0.2),
            A.GridDistortion(p=0.2),
            A.GaussNoise(p=0.2),
            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),
            A.ColorJitter(p=0.3),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    else:  # val or test
        return A.Compose([
            A.Resize(CONFIG['image_size'], CONFIG['image_size']),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])

# Định nghĩa kiến trúc mô hình nâng cao
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)
        )
        
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out) * x

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        out = self.conv(x_cat)
        return self.sigmoid(out) * x

class CBAM(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_att = ChannelAttention(in_channels, reduction_ratio)
        self.spatial_att = SpatialAttention(kernel_size)
        
    def forward(self, x):
        x = self.channel_att(x)
        x = self.spatial_att(x)
        return x

class ConvBnAct(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, 
                 stride=1, groups=1, act=True, bias=False):
        super(ConvBnAct, self).__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size=kernel_size, padding=padding,
            stride=stride, groups=groups, bias=bias
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = nn.SiLU(inplace=True) if act else nn.Identity()
        
    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.act(x)
        return x

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, drop_path_rate=0.0):
        super(ResidualBlock, self).__init__()
        self.conv1 = ConvBnAct(in_channels, out_channels, kernel_size=3, stride=stride)
        self.conv2 = ConvBnAct(out_channels, out_channels, kernel_size=3, act=False)
        self.shortcut = nn.Identity()
        
        if stride != 1 or in_channels != out_channels:
            self.shortcut = ConvBnAct(in_channels, out_channels, kernel_size=1, 
                                     padding=0, stride=stride, act=False)
        
        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()
        self.act = nn.SiLU(inplace=True)
        self.attention = CBAM(out_channels)
        
    def forward(self, x):
        shortcut = self.shortcut(x)
        
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.attention(x)
        
        x = self.drop_path(x) + shortcut
        x = self.act(x)
        return x

class DecoderBlock(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels, drop_path_rate=0.0):
        super(DecoderBlock, self).__init__()
        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        
        self.decoder_block = nn.Sequential(
            ResidualBlock(out_channels + skip_channels, out_channels, drop_path_rate=drop_path_rate),
            CBAM(out_channels)
        )
        
    def forward(self, x, skip=None):
        x = self.upconv(x)
        
        if skip is not None:
            # Đảm bảo kích thước phù hợp
            if x.shape[2:] != skip.shape[2:]:
                x = nn.functional.interpolate(
                    x, size=skip.shape[2:], mode='bilinear', align_corners=True
                )
            x = torch.cat([x, skip], dim=1)
            
        x = self.decoder_block(x)
        return x

class AdvancedUNet(nn.Module):
    def __init__(self, encoder_name="efficientnet-b4", encoder_weights="imagenet",
                 decoder_attention="cbam", deep_supervision=True, num_classes=1):
        super(AdvancedUNet, self).__init__()
        
        # Sử dụng encoder được nâng cấp từ thư viện segmentation_models_pytorch
        self.encoder = smp.encoders.get_encoder(
            encoder_name, 
            in_channels=3, 
            weights=encoder_weights
        )
        
        encoder_channels = self.encoder.out_channels
        decoder_channels = encoder_channels[1:][::-1]
        
        # Các khối giải mã với cơ chế chú ý
        self.decoder_blocks = nn.ModuleList()
        drop_rates = [0.2, 0.2, 0.1, 0.1, 0.0]
        
        for i in range(len(decoder_channels)):
            if i == 0:
                in_channels = encoder_channels[-1]
            else:
                in_channels = decoder_channels[i-1]
                
            skip_channels = encoder_channels[-i-2] if i < len(encoder_channels)-1 else 0
            out_channels = decoder_channels[i]
            
            self.decoder_blocks.append(
                DecoderBlock(in_channels, skip_channels, out_channels, drop_path_rate=drop_rates[i])
            )
        
        # Đầu ra cho mỗi cấp độ giải mã nếu sử dụng giám sát sâu
        self.deep_supervision = deep_supervision
        if deep_supervision:
            self.segmentation_heads = nn.ModuleList([
                nn.Sequential(
                    nn.Conv2d(decoder_channels[i], num_classes, kernel_size=3, padding=1),
                    nn.UpsamplingBilinear2d(scale_factor=2**(len(decoder_channels)-i))
                )
                for i in range(len(decoder_channels))
            ])
        else:
            self.segmentation_head = nn.Conv2d(decoder_channels[-1], num_classes, kernel_size=1)
            
    def forward(self, x):
        # Lấy feature maps từ encoder
        features = self.encoder(x)
        
        # Đảo ngược thứ tự để xử lý từ bottleneck lên
        encoder_features = features[1:][::-1]
        
        # Feature map cuối cùng từ encoder là đầu vào cho decoder
        decoder_features = [features[-1]]
        
        # Lặp qua các khối decoder
        for i, decoder_block in enumerate(self.decoder_blocks):
            if i == 0:
                # Đầu vào cho khối decoder đầu tiên là feature map cuối cùng của encoder
                x = decoder_block(decoder_features[-1], None)
            else:
                # Đầu vào cho các khối decoder tiếp theo là đầu ra từ khối trước đó 
                # và feature map tương ứng từ encoder (skip connection)
                x = decoder_block(x, encoder_features[i])
            
            decoder_features.append(x)
        
        # Xử lý đầu ra dựa trên mode deep supervision
        if self.deep_supervision:
            # Trả về các dự đoán tại tất cả các cấp độ cho deep supervision
            outputs = [head(feat) for head, feat in zip(self.segmentation_heads, decoder_features[1:])]
            return outputs
        else:
            # Trả về dự đoán cuối cùng
            return self.segmentation_head(decoder_features[-1])

# Hàm mất mát tùy chỉnh
class AdvancedLoss(nn.Module):
    def __init__(self, weights=None):
        super(AdvancedLoss, self).__init__()
        self.bce = nn.BCEWithLogitsLoss(weight=weights['bce'] if weights else None)
        self.dice = DiceLoss(mode='binary')
        self.focal = FocalLoss(mode='binary', alpha=0.25, gamma=2.0)
        self.lovasz = LovaszLoss(mode='binary', per_image=True)
        
        # Cân bằng các thành phần mất mát
        self.weights = weights or {
            'bce': 0.5,
            'dice': 0.3,
            'focal': 0.1,
            'lovasz': 0.1
        }
        
    def forward(self, outputs, targets):
        if isinstance(outputs, list):  # Trường hợp deep supervision
            loss = 0
            for i, output in enumerate(outputs):
                # Áp dụng trọng số lớn hơn cho dự đoán cuối cùng
                weight = 0.5 if i == len(outputs) - 1 else 0.5 / (len(outputs) - 1)
                loss += weight * self._compute_loss(output, targets)
            return loss
        else:
            return self._compute_loss(outputs, targets)
            
    def _compute_loss(self, output, target):
        bce_loss = self.bce(output, target)
        dice_loss = self.dice(output, target)
        focal_loss = self.focal(output, target)
        lovasz_loss = self.lovasz(output, target)
        
        return (
            self.weights['bce'] * bce_loss + 
            self.weights['dice'] * dice_loss + 
            self.weights['focal'] * focal_loss + 
            self.weights['lovasz'] * lovasz_loss
        )

# Test-time augmentation
class TTAWrapper:
    def __init__(self, model):
        self.model = model
        self.transforms = [
            lambda x: x,
            lambda x: torch.flip(x, dims=[-1]),  # Flip horizontal
            lambda x: torch.flip(x, dims=[-2]),  # Flip vertical
            lambda x: torch.rot90(x, dims=[-1, -2]),  # 90 degrees
            lambda x: torch.rot90(x, dims=[-1, -2], k=2),  # 180 degrees
            lambda x: torch.rot90(x, dims=[-1, -2], k=3),  # 270 degrees
        ]
        
        self.inverse_transforms = [
            lambda x: x,
            lambda x: torch.flip(x, dims=[-1]),  # Flip horizontal back
            lambda x: torch.flip(x, dims=[-2]),  # Flip vertical back
            lambda x: torch.rot90(x, dims=[-2, -1]),  # -90 degrees
            lambda x: torch.rot90(x, dims=[-2, -1], k=2),  # -180 degrees
            lambda x: torch.rot90(x, dims=[-2, -1], k=3),  # -270 degrees
        ]
        
    def __call__(self, images):
        self.model.eval()
        with torch.no_grad():
            result = None
            for i, transform in enumerate(self.transforms):
                transformed = transform(images)
                output = self.model(transformed)
                
                # Nếu deep supervision, chỉ lấy dự đoán cuối cùng
                if isinstance(output, list):
                    output = output[-1]
                    
                # Áp dụng biến đổi ngược lại
                output = self.inverse_transforms[i](output)
                
                # Gộp với kết quả
                if result is None:
                    result = output
                else:
                    result += output
                    
            # Lấy trung bình
            return result / len(self.transforms)

# Chức năng huấn luyện nâng cao
def train_k_fold(data_loader_creator, model_creator, fold_count=5):
    # Tạo tập dữ liệu đầy đủ
    full_dataset = AdvancedRoadSegmentationDataset(
        image_dir=image_folder,
        mask_dir=mask_folder,
        transform=None,
        preload=True
    )
    
    # Khởi tạo KFold
    kfold = KFold(n_splits=fold_count, shuffle=True, random_state=CONFIG['seed'])
    
    # Lưu trữ kết quả cho mỗi fold
    fold_results = []
    
    # Thực hiện K-fold cross validation
    for fold, (train_indices, val_indices) in enumerate(kfold.split(range(len(full_dataset)))):
        print(f"===== Đang huấn luyện fold {fold+1}/{fold_count} =====")
        
        # Khởi tạo mô hình mới cho mỗi fold
        model = model_creator().to(device)
        
        # Tạo tập dữ liệu train và validation
        train_dataset = Subset(full_dataset, train_indices)
        val_dataset = Subset(full_dataset, val_indices)
        
        # Áp dụng biến đổi
        train_dataset.dataset.transform = get_transforms("train")
        val_dataset.dataset.transform = get_transforms("val")
        
        # Tạo data loaders
        train_loader = DataLoader(
            train_dataset, 
            batch_size=CONFIG['batch_size'],
            shuffle=True,
            num_workers=CONFIG['num_workers'],
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=CONFIG['batch_size'],
            shuffle=False,
            num_workers=CONFIG['num_workers'],
            pin_memory=True
        )
        
        # Optimizers and schedulers
        optimizer = RAdam(
            model.parameters(), 
            lr=CONFIG['learning_rate'],
            weight_decay=CONFIG['weight_decay']
        )
        
        # Scheduler selection
        if CONFIG['lr_scheduler'] == 'cosine':
            scheduler = CosineAnnealingWarmRestarts(
                optimizer, 
                T_0=CONFIG['num_epochs'] // 3, 
                T_mult=2,
                eta_min=CONFIG['learning_rate'] * 0.01
            )
        else:  # 'onecycle'
            scheduler = OneCycleLR(
                optimizer,
                max_lr=CONFIG['learning_rate'],
                epochs=CONFIG['num_epochs'],
                steps_per_epoch=len(train_loader),
                pct_start=0.3
            )
            
        # Định nghĩa hàm mất mát
        criterion = AdvancedLoss(CONFIG['loss_weights'])
        
        # Scaler cho mixed precision
        scaler = amp.GradScaler(enabled=CONFIG['use_mixed_precision'])
        
        # Lưu theo dõi loss và metrics
        best_val_loss = float('inf')
        early_stop_counter = 0
        train_losses = []
        val_losses = []
        val_dices = []
        
        # Vòng lặp huấn luyện
        for epoch in range(CONFIG['num_epochs']):
            print(f"Bắt đầu epoch {epoch + 1}/{CONFIG['num_epochs']}...")
            
            # ===== Huấn luyện =====
            model.train()
            running_loss = 0.0
            train_iterator = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['num_epochs']} [Train]", unit="batch")
            
            for batch_idx, (images, masks) in enumerate(train_iterator):
                images, masks = images.to(device), masks.to(device)
                
                optimizer.zero_grad()
                
                # Sử dụng mixed precision
                with amp.autocast(enabled=CONFIG['use_mixed_precision']):
                    outputs = model(images)
                    loss = criterion(outputs, masks)
                
                # Thực hiện backpropagation với scaler
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                
                # Cập nhật scheduler nếu là OneCycleLR
                if CONFIG['lr_scheduler'] == 'onecycle':
                    scheduler.step()
                    
                running_loss += loss.item() * images.size(0)
                train_iterator.set_postfix(loss=loss.item())
                
                # Log to W&B
                wandb.log({
                    "train_batch_loss": loss.item(),
                    "learning_rate": optimizer.param_groups[0]['lr']
                })
            
            train_loss = running_loss / len(train_loader.dataset)
            train_losses.append(train_loss)
            
            # ===== Validation =====
            model.eval()
            running_val_loss = 0.0
            dice_metric = Dice(average='macro', num_classes=2).to(device)
            iou_metric = JaccardIndex(task='binary', num_classes=2).to(device)
            
            val_iterator = tqdm(val_loader, desc=f"Epoch {epoch+1}/{CONFIG['num_epochs']} [Val]", unit="batch")
            
            with torch.no_grad():
                for images, masks in val_iterator:
                    images, masks = images.to(device), masks.to(device)
                    
                    # Forward pass
                    outputs = model(images)
                    
                    # Handle deep supervision
                    if isinstance(outputs, list):
                        # Use only the final output for validation metrics
                        outputs_for_metrics = outputs[-1]
                    else:
                        outputs_for_metrics = outputs
                    
                    # Calculate loss
                    val_loss = criterion(outputs, masks)
                    running_val_loss += val_loss.item() * images.size(0)
                    
                    # Calculate metrics
                    preds = torch.sigmoid(outputs_for_metrics) > 0.5
                    dice_metric.update(preds.long(), masks.long())
                    iou_metric.update(preds.long(), masks.long())
                    
                    val_iterator.set_postfix(val_loss=val_loss.item())
            
            validation_loss = running_val_loss / len(val_loader.dataset)
            val_losses.append(validation_loss)
            
            # Calculate metrics
            dice_score = dice_metric.compute()
            iou_score = iou_metric.compute()
            val_dices.append(dice_score.item())
            
            print(f'Epoch {epoch + 1} completed. Training Loss: {train_loss:.4f}, '
                  f'Validation Loss: {validation_loss:.4f}, '
                  f'Dice: {dice_score:.4f}, IoU: {iou_score:.4f}')
            
            # Log to W&B
            wandb.log({
                "train_loss": train_loss,
                "val_loss": validation_loss,
                "dice_score": dice_score,
                "iou_score": iou_score,
                "epoch": epoch
            })
            
            # Cập nhật scheduler nếu không phải là OneCycleLR
            if CONFIG['lr_scheduler'] == 'cosine':
                scheduler.step()
            
            # Lưu mô hình tốt nhất
            if validation_loss < best_val_loss:
                best_val_loss = validation_loss
                early_stop_counter = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': validation_loss,
                    'dice_score': dice_score.item(),
                    'iou_score': iou_score.item()
                }, f'best_model_fold_{fold}.pth')
                print(f"Mô hình tốt nhất đã được lưu với validation loss: {best_val_loss:.4f}")
            else:
                early_stop_counter += 1
                print(f"Validation loss không cải thiện. Early stop counter: {early_stop_counter}/{CONFIG['early_stop_patience']}")
            
            # Early stopping
            if early_stop_counter >= CONFIG['early_stop_patience']:
                print(f"Early stopping kích hoạt sau {epoch+1} epochs.")
                break
                
            # Reset metrics
            dice_metric.reset()
            iou_metric.reset()
        
        # Kết quả cho fold này
        fold_results.append({
            'fold': fold,
            'best_val_loss': best_val_loss,
            'best_dice': max(val_dices)
        })
        
        # Vẽ loss curves cho fold hiện tại
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title(f'Training and Validation Loss (Fold {fold+1})')
        plt.savefig(f'loss_curves_fold_{fold}.png')
        plt.close()
        
        # Xóa bộ nhớ GPU
        del model, optimizer, scheduler, train_loader, val_loader
        torch.cuda.empty_cache()
    
    # Tính toán và hiển thị kết quả tổng hợp
    avg_val_loss = sum(result['best_val_loss'] for result in fold_results) / len(fold_results)
    avg_dice = sum(result['best_dice'] for result in fold_results) / len(fold_results)
    
    print("\n===== Kết quả K-Fold Cross Validation =====")
    for result in fold_results:
        print(f"Fold {result['fold']+1}: Best Val Loss = {result['best_val_loss']:.4f}, Best Dice = {result['best_dice']:.4f}")
    print(f"Trung bình: Val Loss = {avg_val_loss:.4f}, Dice = {avg_dice:.4f}")
    
    return fold_results

# Đánh giá mô hình với Test-Time Augmentation
def evaluate_model(model, test_loader, device, use_tta=True):
    if use_tta:
        model_wrapper = TTAWrapper(model)
    else:
        model_wrapper = model
    
    model_wrapper.eval()
    
    # Khởi tạo metrics
    dice_metric = Dice(average='macro', num_classes=2).to(device)
    iou_metric = JaccardIndex(task='binary', num_classes=2).to(device)
    
    # Không cần gradient cho đánh giá
    with torch.no_grad():
        test_iterator = tqdm(test_loader, desc="Đang đánh giá", unit="batch")
        for images, masks in test_iterator:
            images, masks = images.to(device), masks.to(device)
            
            # Lấy dự đoán mô hình
            outputs = model_wrapper(images)
            
            # Xử lý nếu deep supervision
            if isinstance(outputs, list):
                outputs = outputs[-1]
                
            # Áp dụng sigmoid và ngưỡng để lấy mặt nạ nhị phân
            predictions = (torch.sigmoid(outputs) > 0.5).long()
            
            if predictions.shape != masks.shape:
                masks = masks.long()
            
            # Cập nhật metrics
            dice_metric.update(predictions, masks.long())
            iou_metric.update(predictions, masks.long())
    
    # Tính toán metrics cuối cùng
    avg_dice = dice_metric.compute()
    avg_iou = iou_metric.compute()
    
    # Reset metrics cho lần sử dụng tiếp theo
    dice_metric.reset()
    iou_metric.reset()
    
    print(f"Hệ số Dice trung bình: {avg_dice:.4f}")
    print(f"IoU trung bình: {avg_iou:.4f}")
    
    # Log to W&B
    wandb.log({
        "test_dice": avg_dice,
        "test_iou": avg_iou
    })
    
    return avg_dice, avg_iou

# Trực quan hóa kết quả phân đoạn
def visualize_results(model, test_loader, device, num_images=5, use_tta=True):
    if use_tta:
        model_wrapper = TTAWrapper(model)
    else:
        model_wrapper = model
        
    model_wrapper.eval()
    
    # Lấy ngẫu nhiên các mẫu từ test_loader
    data_iter = iter(test_loader)
    samples = [(next(data_iter)) for _ in range(min(num_images, len(test_loader)))]
    
    fig, axes = plt.subplots(num_images, 4, figsize=(20, 5 * num_images))
    for idx, (images, masks) in enumerate(samples):
        images, masks = images.to(device), masks.to(device)
        
        # Lấy dự đoán mô hình
        with torch.no_grad():
            outputs = model_wrapper(images)
            
            # Xử lý nếu deep supervision
            if isinstance(outputs, list):
                outputs = outputs[-1]
                
            preds = (torch.sigmoid(outputs) > 0.5).float()
        
        # Chọn ảnh đầu tiên từ batch
        image = images[0].cpu().permute(1, 2, 0).numpy()
        mask = masks[0].squeeze().cpu().numpy()
        pred = preds[0].squeeze().cpu().numpy()
        
        # Chuẩn hóa hình ảnh cho hiển thị
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = std * image + mean
        image = np.clip(image, 0, 1)
        
        # Tạo overlay cho dự đoán
        pred_colored = np.zeros_like(image)
        pred_colored[..., 0] = pred  # Kênh đỏ cho dự đoán
        
        # Tạo overlay cho ground truth
        mask_colored = np.zeros_like(image)
        mask_colored[..., 1] = mask  # Kênh xanh lá cho mặt nạ thực tế
        
        # Tạo overlay kết hợp
        combined = image.copy()
        combined[pred == 1] = [1, 0.3, 0.3]  # Đỏ cho dự đoán dương tính
        combined[mask == 1] = [0.3, 1, 0.3]  # Xanh lá cho ground truth dương tính
        combined[(pred == 1) & (mask == 1)] = [1, 1, 0.3]  # Vàng cho vùng chồng lấp
        
        # Hiển thị
        axes[idx, 0].imshow(image)
        axes[idx, 0].set_title("Ảnh gốc")
        axes[idx, 0].axis("off")
        
        axes[idx, 1].imshow(mask, cmap='gray')
        axes[idx, 1].set_title("Ground Truth")
        axes[idx, 1].axis("off")
        
        axes[idx, 2].imshow(pred, cmap='gray')
        axes[idx, 2].set_title("Dự đoán")
        axes[idx, 2].axis("off")
        
        axes[idx, 3].imshow(combined)
        axes[idx, 3].set_title("Overlay: Thực tế (xanh), Dự đoán (đỏ), Chồng lấp (vàng)")
        axes[idx, 3].axis("off")
    
    plt.tight_layout()
    plt.savefig('segmentation_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Log to W&B
    wandb.log({"segmentation_results": wandb.Image(plt)})

# Chính sách kiểm nghiệm
def create_model():
    return AdvancedUNet(
        encoder_name=CONFIG['encoder_name'],
        encoder_weights="imagenet",
        decoder_attention="cbam",
        deep_supervision=CONFIG['use_deep_supervision'],
        num_classes=1
    )

# Chạy K-fold Cross Validation
print("Bắt đầu huấn luyện K-fold...")
fold_results = train_k_fold(None, create_model, fold_count=CONFIG['k_folds'])

# Tạo mô hình cuối cùng với toàn bộ dữ liệu
print("Huấn luyện mô hình cuối cùng với toàn bộ dữ liệu...")
full_dataset = AdvancedRoadSegmentationDataset(
    image_dir=image_folder,
    mask_dir=mask_folder,
    transform=get_transforms("train"),
    preload=True
)

# Chia tập dữ liệu thành train và test
train_size = int(0.85 * len(full_dataset))
test_size = len(full_dataset) - train_size

train_dataset, test_dataset = random_split(
    full_dataset,
    [train_size, test_size],
    generator=torch.Generator().manual_seed(CONFIG['seed'])
)

# Áp dụng biến đổi
test_dataset.dataset.transform = get_transforms("test")

# Tạo data loaders
train_loader = DataLoader(
    train_dataset,
    batch_size=CONFIG['batch_size'],
    shuffle=True,
    num_workers=CONFIG['num_workers'],
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=CONFIG['batch_size'],
    shuffle=False,
    num_workers=CONFIG['num_workers'],
    pin_memory=True
)

# Khởi tạo mô hình cuối cùng
final_model = create_model().to(device)

# Optimizers
optimizer = RAdam(
    final_model.parameters(),
    lr=CONFIG['learning_rate'],
    weight_decay=CONFIG['weight_decay']
)

# Scheduler
if CONFIG['lr_scheduler'] == 'cosine':
    scheduler = CosineAnnealingWarmRestarts(
        optimizer,
        T_0=CONFIG['num_epochs'] // 3,
        T_mult=2,
        eta_min=CONFIG['learning_rate'] * 0.01
    )
else:  # 'onecycle'
    scheduler = OneCycleLR(
        optimizer,
        max_lr=CONFIG['learning_rate'],
        epochs=CONFIG['num_epochs'],
        steps_per_epoch=len(train_loader),
        pct_start=0.3
    )

# Định nghĩa hàm mất mát
criterion = AdvancedLoss(CONFIG['loss_weights'])

# Scaler cho mixed precision
scaler = amp.GradScaler(enabled=CONFIG['use_mixed_precision'])

# Vòng lặp huấn luyện cho mô hình cuối cùng
best_val_loss = float('inf')
early_stop_counter = 0
train_losses = []
val_losses = []

for epoch in range(CONFIG['num_epochs']):
    print(f"Bắt đầu epoch {epoch + 1}/{CONFIG['num_epochs']}...")
    
    # ===== Huấn luyện =====
    final_model.train()
    running_loss = 0.0
    train_iterator = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['num_epochs']} [Train]", unit="batch")
    
    for batch_idx, (images, masks) in enumerate(train_iterator):
        images, masks = images.to(device), masks.to(device)
        
        optimizer.zero_grad()
        
        # Sử dụng mixed precision
        with amp.autocast(enabled=CONFIG['use_mixed_precision']):
            outputs = final_model(images)
            loss = criterion(outputs, masks)
        
        # Thực hiện backpropagation với scaler
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # Cập nhật scheduler nếu là OneCycleLR
        if CONFIG['lr_scheduler'] == 'onecycle':
            scheduler.step()
            
        running_loss += loss.item() * images.size(0)
        train_iterator.set_postfix(loss=loss.item())
        
        # Log to W&B
        wandb.log({
            "final_train_batch_loss": loss.item(),
            "final_learning_rate": optimizer.param_groups[0]['lr']
        })
    
    train_loss = running_loss / len(train_loader.dataset)
    train_losses.append(train_loss)
    
    # Cập nhật scheduler nếu không phải là OneCycleLR
    if CONFIG['lr_scheduler'] == 'cosine':
        scheduler.step()
    
    print(f'Epoch {epoch + 1} completed. Training Loss: {train_loss:.4f}')
    
    # Log to W&B
    wandb.log({
        "final_train_loss": train_loss,
        "final_epoch": epoch
    })
    
    # Lưu mô hình cuối cùng sau mỗi 5 epochs
    if (epoch + 1) % 5 == 0:
        torch.save({
            'epoch': epoch,
            'model_state_dict': final_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': train_loss
        }, f'final_model_epoch_{epoch+1}.pth')

# Lưu mô hình cuối cùng
torch.save({
    'model_state_dict': final_model.state_dict(),
    'config': CONFIG
}, 'final_model.pth')

# Đánh giá mô hình cuối cùng
print("Đánh giá mô hình cuối cùng...")
avg_dice, avg_iou = evaluate_model(
    final_model, 
    test_loader, 
    device, 
    use_tta=CONFIG['use_test_time_augmentation']
)

# Trực quan hóa kết quả
visualize_results(
    final_model, 
    test_loader, 
    device, 
    num_images=5, 
    use_tta=CONFIG['use_test_time_augmentation']
)

# Kết thúc W&B run
wandb.finish()

print("Quá trình huấn luyện đã hoàn tất!")
